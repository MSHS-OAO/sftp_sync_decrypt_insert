---
title: "SFTP_Sync_Decrypt_Insert"
output: html_document
date: "`r Sys.time()`"
---

```{css, echo = FALSE}
caption {
      color: red;
      font-weight: bold
      font-size: 13;
    }
```

```{r libraries constants, include=F}
library(dplyr)
library(tidyr)
library(glue)
library(readxl)
library(writexl)
library(kableExtra)
library(odbc)
library(DBI)
library(dbplyr)
library(doParallel)

dev_dsn <- "OAO Cloud DB Staging"
prod_dsn <- "OAO Cloud DB Production"

# function to convert each record of df to insert statement
get_values <- function(source_table, destination_schema, destination_table) {
  
  PARTNER                   <- source_table[1]
  HOME_FACILITY             <- source_table[2]
  HOME_DEPARTMENT           <- source_table[3]
  WORKED_FACILITY           <- source_table[4]
  WORKED_DEPARTMENT         <- source_table[5]
  START_DATE                <- source_table[6]
  END_DATE                  <- source_table[7]
  EMPLOYEE_ID               <- source_table[8]
  EMPLOYEE_NAME             <- source_table[9]
  APPROVED_HOURS            <- source_table[10]
  POSITION_CODE             <- source_table[11]
  JOBCODE                   <- source_table[12]
  PAYCODE                   <- source_table[13]
  WD_HOURS                  <- source_table[14]
  WD_EXPENSE                <- source_table[15]
  HOME_DEPARTMENT_NAME      <- source_table[16]
  WORKED_DEPARTMENT_NAME    <- source_table[17]
  POSITION_CODE_DESCRIPTION <- source_table[18]
  LOCATION_DESCRIPTION      <- source_table[19]
  WD_COFT                   <- source_table[20]
  WD_ACCOUNT                <- source_table[21]
  WD_LOCATION               <- source_table[22]
  WD_DEPARTMENT             <- source_table[23]
  WD_FUND_NUMBER            <- source_table[24]
  HD_COFT                   <- source_table[25]
  HD_LOCATION               <- source_table[26]
  HD_DEPARTMENT             <- source_table[27]
  WD_COA                    <- source_table[28]
  HD_COA                    <- source_table[29]
  PAYROLL_NAME              <- source_table[30]
  REVERSE_MAP_WORKED        <- source_table[31]
  REVERSE_MAP_HOME          <- source_table[32]
  FILE_NAME                 <- source_table[33]
  
  values <- glue("INTO {glue::double_quote(destination_schema)}.{glue::double_quote(destination_table)}
                 (PARTNER, HOME_FACILITY, HOME_DEPARTMENT, WORKED_FACILITY, 
                 WORKED_DEPARTMENT, START_DATE, END_DATE, EMPLOYEE_ID, 
                 EMPLOYEE_NAME, APPROVED_HOURS, POSITION_CODE, JOBCODE, PAYCODE,
                 WD_HOURS, WD_EXPENSE, HOME_DEPARTMENT_NAME, 
                 WORKED_DEPARTMENT_NAME, POSITION_CODE_DESCRIPTION, 
                 LOCATION_DESCRIPTION, WD_COFT, WD_ACCOUNT, WD_LOCATION, 
                 WD_DEPARTMENT, WD_FUND_NUMBER, HD_COFT, HD_LOCATION, 
                 HD_DEPARTMENT, WD_COA, HD_COA, PAYROLL_NAME, 
                 REVERSE_MAP_WORKED, REVERSE_MAP_HOME, FILE_NAME)
                 VALUES ('{PARTNER}', '{HOME_FACILITY}', '{HOME_DEPARTMENT}',
                 '{WORKED_FACILITY}', '{WORKED_DEPARTMENT}', 
                 TO_DATE('{START_DATE}', 'yyyy-mm-dd'), 
                 TO_DATE('{END_DATE}', 'yyyy-mm-dd'), 
                 '{EMPLOYEE_ID}', '{EMPLOYEE_NAME}', '{APPROVED_HOURS}', 
                 '{POSITION_CODE}', '{JOBCODE}', '{PAYCODE}', '{WD_HOURS}',
                 '{WD_EXPENSE}', '{HOME_DEPARTMENT_NAME}', 
                 '{WORKED_DEPARTMENT_NAME}', '{POSITION_CODE_DESCRIPTION}',
                 '{LOCATION_DESCRIPTION}', '{WD_COFT}', '{WD_ACCOUNT}',
                 '{WD_LOCATION}', '{WD_DEPARTMENT}', '{WD_FUND_NUMBER}',
                 '{HD_COFT}', '{HD_LOCATION}', '{HD_DEPARTMENT}', '{WD_COA}',
                 '{HD_COA}', '{PAYROLL_NAME}', '{REVERSE_MAP_WORKED}', 
                 '{REVERSE_MAP_HOME}', '{FILE_NAME}')")
  
  return(values)
}

get_values_repdef <- function(source_table, destination_schema, destination_table) {
  
  DEFINITION_CODE         <- source_table[1]
  DEFINITION_NAME         <- source_table[2]
  KEY_VOLUME              <- source_table[3]
  LEGACY_COST_CENTER      <- source_table[4]
  ORACLE_COST_CENTER      <- source_table[5]
  COST_CENTER_DESCRIPTION <- source_table[6]
  SITE                    <- source_table[7]
  CORPORATE_SERVICE_LINE  <- source_table[8]
  CLOSED                  <- source_table[9]
  VP                      <- source_table[10]
  FTE_TREND               <- source_table[11]
  DEPARTMENT_BREAKDOWN    <- source_table[12]
  
  values <- glue("INTO {glue::double_quote(destination_schema)}.{glue::double_quote(destination_table)}
                 (DEFINITION_CODE, DEFINITION_NAME, KEY_VOLUME, 
                  LEGACY_COST_CENTER, ORACLE_COST_CENTER, 
                  COST_CENTER_DESCRIPTION, SITE, CORPORATE_SERVICE_LINE, CLOSED,
                  VP, FTE_TREND, DEPARTMENT_BREAKDOWN)
                 VALUES ('{DEFINITION_CODE}', '{DEFINITION_NAME}', 
                  '{KEY_VOLUME}', '{LEGACY_COST_CENTER}', '{ORACLE_COST_CENTER}', 
                  '{COST_CENTER_DESCRIPTION}', '{SITE}', '{CORPORATE_SERVICE_LINE}',
                 TO_DATE('{CLOSED}', 'yyyy-mm-dd'), '{VP}', '{FTE_TREND}', 
                 '{DEPARTMENT_BREAKDOWN}')")
  
  return(values)
}

get_values_dates <- function(source_table, destination_schema, destination_table) {
  
  RAW_FILE     <- source_table[1]
  DECRYPT_FILE <- source_table[2]
  INSERT_FILE  <- source_table[3]
  RAW_MTIME    <- source_table[4]
  START_DATE   <- source_table[5]
  END_DATE     <- source_table[6]
  
  values <- glue("INTO {glue::double_quote(destination_schema)}.{glue::double_quote(destination_table)}
                 (RAW_FILE, DECRYPT_FILE, INSERT_FILE, RAW_MTIME, START_DATE, END_DATE)
                 VALUES ('{RAW_FILE}', '{DECRYPT_FILE}', '{INSERT_FILE}', 
                 TO_DATE('{RAW_MTIME}', 'yyyy-mm-dd hh24:mi:ss'), '{START_DATE}', '{END_DATE}')")
  
  return(values)
}
```

# SFTP Synchronization
```{r sftp_sync, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

if (length(paste(list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Raw/")),
                 list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Raw/")))) != 0) {
  # get files currently in network directory
  mshq_raw_files <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Raw/"))
  bislr_raw_files <- list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Raw/"))
  
  # establish connection to OAO oracle cloud db
  con_dev <- dbConnect(odbc(), dev_dsn)
  
  # get current MSHQ and BISLR file and date mapping file
  mshq_dates <- tbl(con_dev, "LPM_FILE_DATES_MSHQ") %>%
    collect()
  bislr_dates <- tbl(con_dev, "LPM_FILE_DATES_BISLR") %>%
    collect()
  
  # run lftp file to sync sftp to network drive
  system('lftp -f /data/SFTP/sync_to_network.lftp')
  
  # get new files in network drive
  mshq_raw_files_new <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Raw/"))
  bislr_raw_files_new <- list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Raw/"))
  
  #print new files that were copied over in sync
  new_files <- setdiff(c(mshq_raw_files_new, bislr_raw_files_new),
                       c(mshq_raw_files, bislr_raw_files))
  if (length(new_files) > 0) {
    cat(cat(paste0(new_files, "\n")), "have been added to the network drive", fill = TRUE)
  } else {
    cat("Network drive is already up to date")
  }
  
  # add any new files to the mshq dates files
  mshq_raw_files_info <- file.info(list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Raw/"), 
                                         full.names = T))
  mshq_raw_files_info <- data.frame(RAW_FILE = basename(rownames(mshq_raw_files_info)),
                                    RAW_MTIME = mshq_raw_files_info$mtime)
  mshq_dates <- mshq_raw_files_info %>%
    left_join(mshq_dates, by = join_by(RAW_FILE, RAW_MTIME)) %>%
    select(RAW_FILE, DECRYPT_FILE, INSERT_FILE, RAW_MTIME, START_DATE, END_DATE) %>%
    arrange(RAW_MTIME) %>%
    mutate(RAW_MTIME = as.character(RAW_MTIME))
  
  # add any new files to the bislr dates files
  bislr_raw_files_info <- file.info(list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Raw/"), 
                                         full.names = T))
  bislr_raw_files_info <- data.frame(RAW_FILE = basename(rownames(bislr_raw_files_info)),
                                    RAW_MTIME = bislr_raw_files_info$mtime)
  bislr_dates <- bislr_raw_files_info %>%
    left_join(bislr_dates, by = join_by(RAW_FILE, RAW_MTIME)) %>%
    select(RAW_FILE, DECRYPT_FILE, INSERT_FILE, RAW_MTIME, START_DATE, END_DATE) %>%
    arrange(RAW_MTIME) %>%
    mutate(RAW_MTIME = as.character(RAW_MTIME))
  
  
    # send new dates files to DB
    dates_schema <- "OAO_DEVELOPMENT"
    dates_table_mshq <- "LPM_FILE_DATES_MSHQ"
    dates_table_bislr <- "LPM_FILE_DATES_BISLR"
    
    # truncate query
    dates_truncate_mshq <- glue("TRUNCATE TABLE LPM_FILE_DATES_MSHQ")
    dates_truncate_bislr <- glue("TRUNCATE TABLE LPM_FILE_DATES_BISLR")
    
    #replace NA
    mshq_dates <- mshq_dates %>%
      mutate(across(everything(), as.character)) %>%
      mutate(RAW_FILE = replace_na(RAW_FILE, ""),
             DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
             INSERT_FILE = replace_na(INSERT_FILE, ""),
             RAW_MTIME = replace_na(RAW_MTIME, ""),
             START_DATE = replace_na(START_DATE, ""),
             END_DATE = replace_na(END_DATE, ""))
  
    bislr_dates <- bislr_dates %>%
      mutate(across(everything(), as.character)) %>%
      mutate(RAW_FILE = replace_na(RAW_FILE, ""),
             DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
             INSERT_FILE = replace_na(INSERT_FILE, ""),
             RAW_MTIME = replace_na(RAW_MTIME, ""),
             START_DATE = replace_na(START_DATE, ""),
             END_DATE = replace_na(END_DATE, ""))
    
    # convert the each record/row of tibble to INTO clause of insert statment
    inserts_dates_mshq <- 
      lapply(
        lapply(
          lapply(split(mshq_dates, 
                       1:nrow(mshq_dates)),
                 as.list),
          as.character),
        FUN = get_values_dates, dates_schema, dates_table_mshq)
    
    inserts_dates_bislr <- 
      lapply(
        lapply(
          lapply(split(bislr_dates, 
                       1:nrow(bislr_dates)),
                 as.list),
          as.character),
        FUN = get_values_dates, dates_schema, dates_table_bislr)
    
    # colapse list elements into indvidual insert statements
    values_dates_mshq <- glue_collapse(inserts_dates_mshq, sep = "\n\n")
    values_dates_bislr <- glue_collapse(inserts_dates_bislr, sep = "\n\n")
    
    # combine all insert statements into an insert all statement
    all_data_mshq_dates <- glue('INSERT ALL
                          {values_dates_mshq}
                        SELECT 1 from DUAL;')
    all_data_bislr_dates <- glue('INSERT ALL
                          {values_dates_bislr}
                        SELECT 1 from DUAL;')
    
    # truncate destination tables before inserts
    con_dev <- dbConnect(odbc(), dev_dsn)
    dbBegin(con_dev)
    tryCatch({
      dbExecute(con_dev, dates_truncate_mshq)
      dbExecute(con_dev, dates_truncate_bislr)
      dbExecute(con_dev, all_data_mshq_dates)
      dbExecute(con_dev, all_data_bislr_dates)
      dbCommit(con_dev)
      dbDisconnect(con_dev)
      print("File Dates tables have been updated")
    }, 
    error = function(err){
      dbRollback(con_dev)
      dbDisconnect(con_dev)
      print("Error")
    }
    )
  
  print("sftp_sync successful")
    
} else {
  stop("Network drive is not mounted on server")
  knitr::knit_exit()
}

```

# Decryption
#### Import Decryption Key
```{r import_key, echo=FALSE}
system('cat /data/SFTP/MSH_ALLINTERNAL_OB_PRIVATE.asc | gpg --import --no-tty --batch --yes')
print("import_key successful")
```

#### MSHQ Decryption
```{r mshq_decrypt, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# get current MSHQ date file
con_dev <- dbConnect(odbc(), dev_dsn)
mshq_dates <- tbl(con_dev, "LPM_FILE_DATES_MSHQ") %>%
  collect()

# list files in the raw and decryption directories
decrypt_files <- list.files(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/"))

# list files that need to be decrypted
files_to_decrypt <- mshq_dates %>%
  filter(!(gsub(".out", "", RAW_FILE) %in% decrypt_files)) %>%
  select(RAW_FILE) %>%
  pull()
  
# decrypt each file in raw folder and not in the decrypted folder
decrypt_function <- lapply(files_to_decrypt, function(x) {
  encrypted <- paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Raw/", x)
  decrypted <- paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/", gsub(".out", "", x))
  system(glue("gpg --output '{decrypted}' --decrypt --pinentry-mode loopback --passphrase-file /data/SFTP/password.txt --batch '{encrypted}'"))
  }
)

# get new files in network drive
decrypt_files_new <- list.files(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/"))

# print new files that were decrypted
new_files <- setdiff(decrypt_files_new, decrypt_files)
if (length(new_files) > 0) {
  cat(paste(new_files, "has been decrypted"))
  
  # add any new files to the mshq dates files
  mshq_dates <- mshq_dates %>%
    mutate(DECRYPT_FILE = case_when(
      is.na(DECRYPT_FILE) ~ gsub(".out", "", RAW_FILE),
      TRUE ~ DECRYPT_FILE)) %>%
    mutate(RAW_MTIME = as.character(RAW_MTIME))
  
  # send new dates files to DB
  dates_schema <- "OAO_DEVELOPMENT"
  dates_table_mshq <- "LPM_FILE_DATES_MSHQ"
  
  # truncate query
  dates_truncate_mshq <- glue("TRUNCATE TABLE LPM_FILE_DATES_MSHQ")
  
  # replace NA
  mshq_dates <- mshq_dates %>%
    mutate(across(everything(), as.character)) %>%
    mutate(RAW_FILE = replace_na(RAW_FILE, ""),
           DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
           INSERT_FILE = replace_na(INSERT_FILE, ""),
           RAW_MTIME = replace_na(RAW_MTIME, ""),
           START_DATE = replace_na(START_DATE, ""),
           END_DATE = replace_na(END_DATE, ""))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts_dates_mshq <- 
    lapply(
      lapply(
        lapply(split(mshq_dates, 
                     1:nrow(mshq_dates)),
               as.list),
        as.character),
      FUN = get_values_dates, dates_schema, dates_table_mshq)
  
  # colapse list elements into indvidual insert statements
  values_dates_mshq <- glue_collapse(inserts_dates_mshq, sep = "\n\n")
  
  # combine all insert statements into an insert all statement
  all_data_mshq_dates <- glue('INSERT ALL
                        {values_dates_mshq}
                      SELECT 1 from DUAL;')
  
  # truncate destination tables before inserts
  con_dev <- dbConnect(odbc(), dev_dsn)
  dbBegin(con_dev)
  tryCatch({
    dbExecute(con_dev, dates_truncate_mshq)
    dbExecute(con_dev, all_data_mshq_dates)
    dbCommit(con_dev)
    dbDisconnect(con_dev)
    print("MSHQ File Dates table has been updated")
  }, 
  error = function(err){
    dbRollback(con_dev)
    dbDisconnect(con_dev)
    print("Error")
  }
  )
} else {
  cat("No new MSHQ files to decrypt")
}

print("mshq_decrypt successful")
```

#### BISLR Decryption
```{r bislr_decrypt, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# get current MSHQ date file
con_dev <- dbConnect(odbc(), dev_dsn)
bislr_dates <- tbl(con_dev, "LPM_FILE_DATES_BISLR") %>%
  collect()

# list files in the raw and decryption directories
decrypt_files <- list.files(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/"))

# list files that need to be decrypted
files_to_decrypt <- bislr_dates %>%
  filter(!(gsub(".out", "", RAW_FILE) %in% decrypt_files)) %>%
  select(RAW_FILE) %>%
  pull()
  
# decrypt each file in raw folder and not in the decrypted folder
decrypt_function <- lapply(files_to_decrypt, function(x) {
  encrypted <- paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Raw/", x)
  decrypted <- paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/", gsub(".out", "", x))
  system(glue("gpg --output '{decrypted}' --decrypt --pinentry-mode loopback --passphrase-file /data/SFTP/password.txt --batch '{encrypted}'"))
  }
)

# get new files in network drive
decrypt_files_new <- list.files(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/"))

# print new files that were decrypted
new_files <- setdiff(decrypt_files_new, decrypt_files)
if (length(new_files) > 0) {
  cat(paste(new_files, "has been decrypted"))
  
  # add any new files to the mshq dates files
  bislr_dates <- bislr_dates %>%
    mutate(DECRYPT_FILE = case_when(
      is.na(DECRYPT_FILE) ~ gsub(".out", "", RAW_FILE),
      TRUE ~ DECRYPT_FILE)) %>%
    mutate(RAW_MTIME = as.character(RAW_MTIME))
  
  # send new dates files to DB
  dates_schema <- "OAO_DEVELOPMENT"
  dates_table_bislr <- "LPM_FILE_DATES_BISLR"
  
  # truncate query
  dates_truncate_bislr <- glue("TRUNCATE TABLE LPM_FILE_DATES_BISLR")
  
  # replace NA
  bislr_dates <- bislr_dates %>%
    mutate(across(everything(), as.character)) %>%
    mutate(RAW_FILE = replace_na(RAW_FILE, ""),
           DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
           INSERT_FILE = replace_na(INSERT_FILE, ""),
           RAW_MTIME = replace_na(RAW_MTIME, ""),
           START_DATE = replace_na(START_DATE, ""),
           END_DATE = replace_na(END_DATE, ""))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts_dates_bislr <- 
    lapply(
      lapply(
        lapply(split(bislr_dates, 
                     1:nrow(bislr_dates)),
               as.list),
        as.character),
      FUN = get_values_dates, dates_schema, dates_table_bislr)
  
  # colapse list elements into indvidual insert statements
  values_dates_bislr <- glue_collapse(inserts_dates_bislr, sep = "\n\n")
  
  # combine all insert statements into an insert all statement
  all_data_bislr_dates <- glue('INSERT ALL
                        {values_dates_bislr}
                      SELECT 1 from DUAL;')
  
  # truncate destination tables before inserts
  con_dev <- dbConnect(odbc(), dev_dsn)
  dbBegin(con_dev)
  tryCatch({
    dbExecute(con_dev, dates_truncate_bislr)
    dbExecute(con_dev, all_data_bislr_dates)
    dbCommit(con_dev)
    dbDisconnect(con_dev)
    print("File Dates tables have been updated")
  }, 
  error = function(err){
    dbRollback(con_dev)
    dbDisconnect(con_dev)
    print("Error")
  }
  )
} else {
  cat("No new BISLR files to decrypt")
}

print("bislr_decrypt successful")
```

# Insert Creation
#### MSHQ Inserts
```{r mshq_insert, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# col names for Oracle DB
db_columns <- c("PARTNER", "HOME_FACILITY", "HOME_DEPARTMENT", "WORKED_FACILITY", 
                "WORKED_DEPARTMENT", "START_DATE", "END_DATE", "EMPLOYEE_ID",
                "EMPLOYEE_NAME", "APPROVED_HOURS", "POSITION_CODE", "JOBCODE",
                "PAYCODE", "WD_HOURS", "WD_EXPENSE", "HOME_DEPARTMENT_NAME",
                "WORKED_DEPARTMENT_NAME", "POSITION_CODE_DESCRIPTION",
                "LOCATION_DESCRIPTION", "WD_COFT", "WD_ACCOUNT", "WD_LOCATION",
                "WD_DEPARTMENT", "WD_FUND_NUMBER", "HD_COFT", "HD_LOCATION",
                "HD_DEPARTMENT", "WD_COA", "HD_COA", "PAYROLL_NAME",
                "REVERSE_MAP_WORKED", "REVERSE_MAP_HOME", "FILE_NAME")

# read in mshq dates file 
con_dev <- dbConnect(odbc(), dev_dsn)
mshq_dates <- tbl(con_dev, "LPM_FILE_DATES_MSHQ") %>%
  collect() %>%
  mutate(INSERT_FILE = case_when(
    START_DATE == "IGNORE" ~ "IGNORE",
    is.na(START_DATE) ~ NA,
    TRUE ~ INSERT_FILE))

# get list of files that need dates/Ignore
mshq_dates_new <- mshq_dates %>%
  filter(is.na(START_DATE))

# get list of files in insert folder currently
insert_files <- list.files(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/")) 

# update mshq dates for files with newly added START_DATE and END_DATE values
mshq_dates <- mshq_dates %>% 
  arrange(START_DATE) %>% 
  mutate(INSERT_FILE = case_when(
    is.na(INSERT_FILE) & !is.na(START_DATE) & START_DATE != "IGNORE" ~ paste0(row.names(.), "_", DECRYPT_FILE),
    TRUE ~ INSERT_FILE)) %>%
  arrange(RAW_MTIME)

# get list of files that now have date values but dont have an insert file
new_inserts <- setdiff(mshq_dates %>% 
                         filter(!is.na(INSERT_FILE) & START_DATE != "IGNORE") %>% 
                         select(INSERT_FILE) %>% 
                         pull(),
                       insert_files)

# list of files that need an insert file created
create_inserts <- mshq_dates %>%
  filter(INSERT_FILE %in% new_inserts) %>%
  select(DECRYPT_FILE) %>%
  pull()

# create a new insert file for all decrypted files that need insert files
inserts <- lapply(create_inserts, function(x) {
  data <-  read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Decrypt/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 32), strip.white = TRUE) %>%
    mutate(FILENAME = mshq_dates %>%
             filter(DECRYPT_FILE == x) %>%
             select(INSERT_FILE) %>%
             pull()) %>%
    mutate(Start.Date = format(as.Date(Start.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           End.Date = format(as.Date(End.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           Job.Code = case_when(
             Job.Code == "" ~ "UNKNOWN",
             TRUE ~ Job.Code)) %>%
    filter(Start.Date >= as.Date(pull(filter(mshq_dates, DECRYPT_FILE == x) %>%
                                         select(START_DATE)), format = "%Y-%m-%d"),
           End.Date <= as.Date(pull(filter(mshq_dates, DECRYPT_FILE == x) %>%
                                      select(END_DATE)), format = "%Y-%m-%d"))
})
# apply column names of oracle db to all new insert files
inserts <- lapply(inserts, setNames, db_columns)
names(inserts) <- create_inserts

insert_function <- lapply(names(inserts), function(x) {
  write.table(inserts[[x]], paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/",
                                   mshq_dates %>% 
                                     filter(DECRYPT_FILE == x) %>%
                                     select(INSERT_FILE) %>%
                                     pull()),
              sep = "~", row.names = FALSE)
})

# list new insert files if there are any
if (length(new_inserts) > 0) {
  cat(cat(paste0(new_inserts, "\n")), "has been added to the Insert folder", fill = TRUE)
} else {
  cat("Insert files are already up to date")
}

# list files that need START_DATE and END_DATE values
if (nrow(mshq_dates_new) > 0) {
  kable(mshq_dates_new, 
        caption = "The following files need START_DATE and END_DATE values in table LPM_FILE_DATES_MSHQ:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
} 

if (length(new_inserts) > 0 | nrow(mshq_dates_new) > 0) {
  # send new dates files to DB
  dates_schema <- "OAO_DEVELOPMENT"
  dates_table_mshq <- "LPM_FILE_DATES_MSHQ"
  
  # truncate query
  dates_truncate_mshq <- glue("TRUNCATE TABLE LPM_FILE_DATES_MSHQ")
  
  #replace NA
  mshq_dates <- mshq_dates %>%
    mutate(across(everything(), as.character)) %>%
    mutate(RAW_FILE = replace_na(RAW_FILE, ""),
           DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
           INSERT_FILE = replace_na(INSERT_FILE, ""),
           RAW_MTIME = replace_na(RAW_MTIME, ""),
           START_DATE = replace_na(START_DATE, ""),
           END_DATE = replace_na(END_DATE, ""))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts_dates_mshq <- 
    lapply(
      lapply(
        lapply(split(mshq_dates, 
                     1:nrow(mshq_dates)),
               as.list),
        as.character),
      FUN = get_values_dates, dates_schema, dates_table_mshq)
  
  # colapse list elements into indvidual insert statements
  values_dates_mshq <- glue_collapse(inserts_dates_mshq, sep = "\n\n")
  
  # combine all insert statements into an insert all statement
  all_data_mshq_dates <- glue('INSERT ALL
                        {values_dates_mshq}
                      SELECT 1 from DUAL;')
  
  # truncate destination tables before inserts
  con_dev <- dbConnect(odbc(), dev_dsn)
  dbBegin(con_dev)
  tryCatch({
    dbExecute(con_dev, dates_truncate_mshq)
    dbExecute(con_dev, all_data_mshq_dates)
    dbCommit(con_dev)
    dbDisconnect(con_dev)
    print("MSHQ File Dates table has been updated")
  }, 
  error = function(err){
    dbRollback(con_dev)
    dbDisconnect(con_dev)
    print("Error")
  }
  )
}

print("mshq_insert successful")
```

#### BISLR Inserts
```{r bislr_insert, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# col names for Oracle DB
db_columns <- c("PARTNER", "HOME_FACILITY", "HOME_DEPARTMENT", "WORKED_FACILITY", 
                "WORKED_DEPARTMENT", "START_DATE", "END_DATE", "EMPLOYEE_ID",
                "EMPLOYEE_NAME", "APPROVED_HOURS", "POSITION_CODE", "JOBCODE",
                "PAYCODE", "WD_HOURS", "WD_EXPENSE", "HOME_DEPARTMENT_NAME",
                "WORKED_DEPARTMENT_NAME", "POSITION_CODE_DESCRIPTION",
                "LOCATION_DESCRIPTION", "WD_COFT", "WD_ACCOUNT", "WD_LOCATION",
                "WD_DEPARTMENT", "WD_FUND_NUMBER", "HD_COFT", "HD_LOCATION",
                "HD_DEPARTMENT", "WD_COA", "HD_COA", "PAYROLL_NAME",
                "REVERSE_MAP_WORKED", "REVERSE_MAP_HOME", "FILE_NAME")

#Names of the weekly paycyle names in the payroll name column in data files
weekly_pc <- c("WEST WEEKLY", "BIB WEEKLY")

# read in bislr dates file 
con_dev <- dbConnect(odbc(), dev_dsn)
bislr_dates <- tbl(con_dev, "LPM_FILE_DATES_BISLR") %>%
  collect() %>%
  mutate(INSERT_FILE = case_when(
    START_DATE == "IGNORE" ~ "IGNORE",
    is.na(START_DATE) ~ NA,
    TRUE ~ INSERT_FILE))

# get list of files that need dates/Ignore
bislr_dates_new <- bislr_dates %>%
  filter(is.na(START_DATE))

# get list of files in insert folder currently
insert_files <- list.files(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/")) 

# update mshq dates for files with newly added START_DATE and END_DATE values
bislr_dates <- bislr_dates %>% 
  arrange(START_DATE) %>% 
  mutate(INSERT_FILE = case_when(
    is.na(INSERT_FILE) & !is.na(START_DATE) & START_DATE != "IGNORE" ~ paste0(row.names(.), "_", DECRYPT_FILE),
    TRUE ~ INSERT_FILE)) %>%
  arrange(RAW_MTIME)

# get list of files that now have date values but dont have an insert file
new_inserts <- setdiff(bislr_dates %>% 
                         filter(!is.na(INSERT_FILE) & START_DATE != "IGNORE") %>% 
                         select(INSERT_FILE) %>% 
                         pull(),
                       insert_files)
# list of files that need an insert file created
create_inserts <- bislr_dates %>%
  filter(INSERT_FILE %in% new_inserts) %>%
  select(DECRYPT_FILE) %>%
  pull()

# create list of start and end dates for all insert files
delete_weekly <- lapply(create_inserts, function(x) {
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 32), strip.white = TRUE) %>%
     mutate(Start.Date = format(as.Date(Start.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           End.Date = format(as.Date(End.Date, format = "%m/%d/%Y"), "%Y-%m-%d")) %>%
    filter(Start.Date >= as.Date(pull(filter(bislr_dates, DECRYPT_FILE == x) %>%
                                         select(START_DATE)), format = "%Y-%m-%d"),
           End.Date <= as.Date(pull(filter(bislr_dates, DECRYPT_FILE == x) %>%
                                      select(END_DATE)), format = "%Y-%m-%d")) %>%
    arrange(Start.Date, End.Date) %>%
    mutate(Start_End = paste0(Start.Date, "_", End.Date)) %>%
    select(Start.Date, End.Date, Start_End) %>%
    distinct() %>%
    filter(Start.Date == max(Start.Date))
})
# assign create_inserts as the name for the delete weekly list elements
names(delete_weekly) <- create_inserts

# create a new insert file for all decrypted files that need insert files
inserts <- lapply(create_inserts, function(x) {
  data <-  read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Decrypt/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 32), strip.white = TRUE) %>%
    mutate(FILENAME = bislr_dates %>%
             filter(DECRYPT_FILE == x) %>%
             select(INSERT_FILE) %>%
             pull()) %>%
    mutate(Start.Date = format(as.Date(Start.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           End.Date = format(as.Date(End.Date, format = "%m/%d/%Y"), "%Y-%m-%d"),
           Job.Code = case_when(
             Job.Code == "" ~ "UNKNOWN",
             TRUE ~ Job.Code)) %>%
    filter(Start.Date >= as.Date(pull(filter(bislr_dates, DECRYPT_FILE == x) %>%
                                         select(START_DATE)), format = "%Y-%m-%d"),
           End.Date <= as.Date(pull(filter(bislr_dates, DECRYPT_FILE == x) %>%
                                      select(END_DATE)), format = "%Y-%m-%d")) %>%
    mutate(Start_End = paste0(Start.Date, "_", End.Date)) %>%
    filter(Start_End != delete_weekly[[x]][,"Start_End"]) %>%
    select(-Start_End)
})
# apply column names of oracle db to all new insert files
inserts <- lapply(inserts, setNames, db_columns)
names(inserts) <- create_inserts

insert_function <- lapply(names(inserts), function(x) {
  write.table(inserts[[x]], paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/",
                        bislr_dates %>% 
                          filter(DECRYPT_FILE == x) %>%
                          select(INSERT_FILE) %>%
                          pull()),
              sep = "~", row.names = FALSE)
})

# list new insert files if there are any
if (length(new_inserts) > 0) {
  cat(cat(paste0(new_inserts, "\n")), "has been added to the Insert folder", fill = TRUE)
} else {
  cat("Insert files are already up to date")
}

# list files that need START_DATE and END_DATE values
if (nrow(bislr_dates_new) > 0) {
  kable(bislr_dates_new, 
        caption = "The following files need START_DATE and END_DATE values in table LPM_FILE_DATES_BISLR:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
} 

if (length(new_inserts) > 0 | nrow(bislr_dates_new) > 0) {
  # send new dates files to DB
  dates_schema <- "OAO_DEVELOPMENT"
  dates_table_bislr <- "LPM_FILE_DATES_BISLR"
  
  # truncate query
  dates_truncate_bislr <- glue("TRUNCATE TABLE LPM_FILE_DATES_BISLR")
  
  #replace NA
  bislr_dates <- bislr_dates %>%
    mutate(across(everything(), as.character)) %>%
    mutate(RAW_FILE = replace_na(RAW_FILE, ""),
           DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
           INSERT_FILE = replace_na(INSERT_FILE, ""),
           RAW_MTIME = replace_na(RAW_MTIME, ""),
           START_DATE = replace_na(START_DATE, ""),
           END_DATE = replace_na(END_DATE, ""))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts_dates_bislr <- 
    lapply(
      lapply(
        lapply(split(bislr_dates, 
                     1:nrow(bislr_dates)),
               as.list),
        as.character),
      FUN = get_values_dates, dates_schema, dates_table_bislr)
  
  # colapse list elements into indvidual insert statements
  values_dates_bislr <- glue_collapse(inserts_dates_bislr, sep = "\n\n")
  
  # combine all insert statements into an insert all statement
  all_data_bislr_dates <- glue('INSERT ALL
                        {values_dates_bislr}
                      SELECT 1 from DUAL;')
  
  # truncate destination tables before inserts
  con_dev <- dbConnect(odbc(), dev_dsn)
  dbBegin(con_dev)
  tryCatch({
    dbExecute(con_dev, dates_truncate_bislr)
    dbExecute(con_dev, all_data_bislr_dates)
    dbCommit(con_dev)
    dbDisconnect(con_dev)
    print("BISLR File Dates table has been updated")
  }, 
  error = function(err){
    dbRollback(con_dev)
    dbDisconnect(con_dev)
    print("Error")
  }
  )
}

print("bislr_insert successful")
```

# Quality Check
#### Job Code Check
```{r jobcode check, echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# get set of unique files in MSHQ database
mshq_db_files <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull

# get set of unique files in BISLR database
bislr_db_files <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()

# get new files in network drive
mshq_inserts <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Insert/"))
bislr_inserts <- list.files(paste0(universal_dir, "/Labor/Raw Data/BISLR_sftp_sync/Insert/"))

# get set of files to check the mapping files with
mshq_new <- setdiff(mshq_inserts, mshq_db_files)
bislr_new <- setdiff(bislr_inserts, bislr_db_files)

# get all jobcodes in mapping file within database
jobcodes <- tbl(con_dev, "LPM_MAPPING_JOBCODE") %>%
  select(JOBCODE, PAYROLL) %>%
  collect()

# read all insert files that need jobcode check and bind them
mshq_check <- lapply(mshq_new, function(x) {
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 33), strip.white = TRUE)
})
mshq_check <- do.call("rbind", mshq_check)

bislr_check <- lapply(bislr_new, function(x) {
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/", x),
                    sep = "~", header = T, stringsAsFactors = F,
                    colClasses = rep("character", 33), strip.white = TRUE)
})
bislr_check <- do.call("rbind", bislr_check)

# save variable for insert checks. If jobcodes need to be resolved then we cant insert


# check jobcodes in new insert files with jobcode mapping file
if (is.null(mshq_check)) {
  mshq_insert_check <- TRUE
  cat("MSHQ Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else if (length(setdiff((mshq_check %>% 
                    select(JOBCODE) %>% pull()),
                   (jobcodes %>% 
                    filter(PAYROLL == 'MSHQ') %>% select(JOBCODE) %>% pull()))) == 0){
  mshq_insert_check <- TRUE
  cat("MSHQ Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else {
  mshq_insert_check <- FALSE
  # create data frame for new jobcodes at MSHQ
  new_mshq_jobcodes <- mshq_check %>%
    filter(!(JOBCODE %in% (jobcodes %>% 
                             filter(PAYROLL == 'MSHQ') %>% 
                             select(JOBCODE) %>% pull))) %>%
    mutate(PAYROLL = "MSHQ",
           PROVIDER = "",
           JOBCODE_PREMIER = "",
           JOBCODE_PREMIER_DESCRIPTION = "") %>%
    rename(JOBCODE_DESCRIPTION = POSITION_CODE_DESCRIPTION) %>%
    select(JOBCODE, PAYROLL, JOBCODE_DESCRIPTION, PROVIDER, JOBCODE_PREMIER, JOBCODE_PREMIER_DESCRIPTION) %>%
    distinct()
  
  # save df with new MSHQ jobcodes
  write_xlsx(new_mshq_jobcodes, paste0(universal_dir, 
                                       "Mapping/sftp_sync_decrypt_insert/JobCode/MSHQ/new_mshq_jobcodes_", 
                                       Sys.Date(), ".xlsx"))
  
  # print kable of new MSHQ jobcodes
  kable(new_mshq_jobcodes, 
        caption = "The following MSHQ jobcodes need to be added to MAPING_JOBCODE:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}


# check jobcodes in new insert files with jobcode mapping file
if (is.null(bislr_check)) {
  bislr_insert_check <- TRUE
  cat("BISLR Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else if (length(setdiff((bislr_check %>% 
                    select(JOBCODE) %>% pull()),
                   (jobcodes %>% 
                    filter(PAYROLL == 'BISLR') %>% select(JOBCODE) %>% pull()))) == 0){
  bislr_insert_check <- TRUE
  cat("BISLR Jobcodes are up to date in LPM_MAPPING_JOBCODE")
} else {
  bislr_insert_check <- FALSE
  # create data frame for new jobcodes at BISLR
  new_bislr_jobcodes <- bislr_check %>%
    filter(!(JOBCODE %in% (jobcodes %>% 
                             filter(PAYROLL == 'BISLR') %>% 
                             select(JOBCODE) %>% pull))) %>%
    mutate(PAYROLL = "BISLR",
           PROVIDER = "",
           JOBCODE_PREMIER = "",
           JOBCODE_PREMIER_DESCRIPTION = "") %>%
    rename(JOBCODE_DESCRIPTION = POSITION_CODE_DESCRIPTION) %>%
    select(JOBCODE, PAYROLL, JOBCODE_DESCRIPTION, PROVIDER, JOBCODE_PREMIER, JOBCODE_PREMIER_DESCRIPTION) %>%
    distinct()
  
  # save df with new BISLR jobcodes
  write_xlsx(new_bislr_jobcodes, paste0(universal_dir, 
                                       "Mapping/sftp_sync_decrypt_insert/JobCode/BISLR/new_bislr_jobcodes_", 
                                       Sys.Date(), ".xlsx"))
  
  # print kable of new BISLR jobcodes
  kable(new_bislr_jobcodes, 
        caption = "The following BISLR jobcodes need to be added to MAPING_JOBCODE:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}

print("Jobcode check successful")
```

#### Pay Code Check
```{r paycode check , echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# Unique MSHQ files in database
mshq_db_files <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()
  
# Unique BISLR files in database
bislr_db_files <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()

# get list of files in insert folder currently
mshq_insert_files <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Insert/"))
bislr_insert_files <- list.files(paste0(universal_dir, "/Labor//Raw Data/BISLR_sftp_sync/Insert/"))

# Get new files
mshq_new <- setdiff(mshq_insert_files, mshq_db_files)
bislr_new <- setdiff(bislr_insert_files, bislr_db_files)

# Read mapping pay code
mapping_paycode <- tbl(con_dev, "LPM_MAPPING_PAYCODE") %>%
                   select(PAYCODE_RAW) %>%
                   distinct() %>%
                   collect() %>%
                   pull()

# read in new insert files and bind them all
mshq_paycodes <- lapply(mshq_new, function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE) 
 })
mshq_paycodes <- do.call("rbind", mshq_paycodes)

bislr_paycodes <- lapply(bislr_new , function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE) 
 })
bislr_paycodes <- do.call("rbind", bislr_paycodes)

new_paycodes <- rbind(mshq_paycodes, bislr_paycodes)

# get new paycodes in all new insert files
if (!is.null(new_paycodes)) {
  new_paycodes <- new_paycodes %>%
         select(FILE_NAME, PAYCODE) %>%
    filter(!(PAYCODE %in% mapping_paycode)) %>%
    distinct()
}

if(is.null(new_paycodes)) {
  cat("MSHS Paycodes are up to date in LPM_MAPPING_PAYCODE")
} else if (nrow(new_paycodes) == 0) {
  cat("MSHS Paycodes are up to date in LPM_MAPPING_PAYCODE")
} else {
  # save new paycodes df
  write_xlsx(new_paycodes, paste0(universal_dir, 
                                  "Mapping/sftp_sync_decrypt_insert/PayCode/new_paycodes_", 
                                       Sys.Date(), ".xlsx"))
  # print new paycodes df
  kable(new_paycodes, 
        caption = "The following MSHS paycodes need to be added to MAPING_PAYCODE:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}

print("Paycode check successful")
```

#### Pay Cycle Check
```{r paycycle check , echo=FALSE}
# set universal data directory
universal_dir <- "/SharedDrive/deans/Presidents/SixSigma/MSHS Productivity/Productivity/Universal Data/"

# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# Unique MSHQ files in database
mshq_db_files <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()
  
# Unique BISLR files in database
bislr_db_files <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE")%>%
  select(FILE_NAME) %>%
  distinct() %>%
  collect() %>%
  pull()

# get list of files in insert folder currently
mshq_insert_files <- list.files(paste0(universal_dir, "/Labor/Raw Data/MSHQ_sftp_sync/Insert/"))
bislr_insert_files <- list.files(paste0(universal_dir, "/Labor//Raw Data/BISLR_sftp_sync/Insert/"))

# Get new files
mshq_new <- setdiff(mshq_insert_files, mshq_db_files)
bislr_new <- setdiff(bislr_insert_files, bislr_db_files)

# Read mapping paycycle
unique_dates <- tbl(con_dev, "LPM_MAPPING_PAYCYCLE") %>%
                    collect() %>%
                    select(PAYCYCLE_DATE) %>%
                    mutate(PAYCYCLE_DATE = as.Date(PAYCYCLE_DATE)) %>%
                    distinct() %>%
                    pull()

# read new insert files for MSHS and bind them all
mshq_paycycles <- lapply(mshq_new, function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/MSHQ_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE)
})
mshq_paycycles <- do.call("rbind", mshq_paycycles)

bislr_paycycles <- lapply(bislr_new , function(x){
  data <- read.csv(paste0(universal_dir, "Labor/Raw Data/BISLR_sftp_sync/Insert/", x), sep = "~", header = T,
                   stringsAsFactors = F,
                   colClasses = rep("character", 33),
                   strip.white = TRUE) 
})
bislr_paycycles  <- do.call("rbind", bislr_paycycles)

new_paycycles <- rbind(mshq_paycycles, bislr_paycycles)

# get df of all new paycycles
if (!is.null(new_paycycles)) {
  new_paycycles <- new_paycycles %>%
   mutate(END_DATE= as.Date(END_DATE)) %>%
         select(FILE_NAME, END_DATE) %>%
    filter(!(END_DATE %in% unique_dates)) %>%
    distinct()
}

if (is.null(new_paycycles)) {
  cat("MSHS End Dates are included in LPM_MAPPING_PAYCYCLE")
} else if (nrow(new_paycycles) == 0) {
  cat("MSHS End Dates are included in LPM_MAPPING_PAYCYCLE")
} else {
  # save df of new paycycles
  write_xlsx(new_paycycles, paste0(universal_dir, 
                                  "Mapping/sftp_sync_decrypt_insert/PayCycle/new_paycycles_", 
                                       Sys.Date(), ".xlsx"))
  # print new paycycles
  kable(new_paycycles, 
        caption = "The following MSHS End Dates are not included in MAPING_PAYCYCLES:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}

print("Paycycle check successful")
```

#### Insert Check
```{r Insert Check, echo=FALSE}
# establish connection to OAO oracle cloud db
con_dev <- dbConnect(odbc(), dev_dsn)

# mshq check for inserts ready to be uploaded to OAO DB
mshq_eligible_insert <- setdiff(mshq_inserts,
          tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
            select(FILE_NAME) %>%
            distinct() %>%
            collect() %>%
            pull())
if (length(mshq_eligible_insert) > 0 & mshq_insert_check == TRUE) {
  cat(paste0(mshq_eligible_insert, " is ready to be uploaded to the OAO DB"))
} else {
  cat("LPM_DATA_MSHQ_ORACLE is up to date")
}


# bislr check for inserts ready to be uploaded to OAO DB
bislr_eligible_insert <- setdiff(bislr_inserts,
          tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
            select(FILE_NAME) %>%
            distinct() %>%
            collect() %>%
            pull())
if (length(bislr_eligible_insert) > 0 & mshq_insert_check == TRUE) {
  cat(paste0(bislr_eligible_insert, " is ready to be uploaded to the OAO DB"))
} else {
  cat("LPM_DATA_BISLR_ORACLE is up to date")
}

print("Insert check successful")
```

#### End Date Check
```{r End Date Check, echo=FALSE}
# establish DB connection
con_prod <- dbConnect(odbc(), prod_dsn)

# set constants for first end date of each data table
mshq_first_end_date <- as.Date("2020-09-26")
bislr_first_end_date <- as.Date("2021-03-13")

#set constants for threshold check
mshq_weekly <- 450000
mshq_biweekly <- 1400000
bislr_weekly <- 280000
bislr_biweekly <- 600000

# get max end date for each data table
mshq_max_end_date <- as.Date(max(tbl(con_prod, "LPM_DATA_MSHQ_ORACLE") %>%
  select(END_DATE) %>%
  distinct() %>%
  collect() %>%
  pull()))

bislr_max_end_date <- as.Date(max(tbl(con_prod, "LPM_DATA_BISLR_ORACLE") %>%
  select(END_DATE) %>%
  distinct() %>%
  collect() %>%
  pull()))

# set up end date df for MSHQ check
mshq_end_dates <- setNames(data.frame(matrix(ncol = 1, nrow = 0)), c("END_DATE"))
mshq_end_dates <- mshq_end_dates %>% mutate(END_DATE = as.Date(END_DATE))
i_mshq <- mshq_first_end_date
while (i_mshq != mshq_max_end_date) {
  r <- nrow(mshq_end_dates)
  
  mshq_end_dates[r + 1, 1] <- i_mshq
  
  i_mshq <- i_mshq + 7
}

# join mshq end dates with MSHQ data table to get hours for each end date
mshq_end_date_check <- mshq_end_dates %>%
  left_join(tbl(con_prod, "LPM_DATA_MSHQ_ORACLE") %>%
              group_by(END_DATE) %>%
              summarise(HOURS = sum(WD_HOURS, na.rm = TRUE)) %>%
              collect(),
            by = c("END_DATE" = "END_DATE")) %>%
  arrange(END_DATE)

# check for each end date to have hours
if (nrow(filter(mshq_end_date_check, is.na(HOURS))) > 0) {
  mshq_missing_dates <- filter(mshq_end_date_check, is.na(HOURS)) %>%
    select(END_DATE) %>%
    pull()
  
  print(paste(mshq_missing_dates, "do not have hours"))
}

# check that the hours total for each end date is within threshold
mshq_end_date_check <- mshq_end_date_check %>%
  mutate(EXPECTED = as.numeric(NA),
         THRESHOLD = as.numeric(NA))
for (i in 1:nrow(mshq_end_date_check)) {
  if (i %% 2 == 1) {
    mshq_end_date_check[i, "EXPECTED"] <- mshq_biweekly
    mshq_end_date_check[i, "THRESHOLD"] <- 
      mshq_end_date_check[i, "HOURS"] / mshq_biweekly
  } else {
    mshq_end_date_check[i, "EXPECTED"] <- mshq_weekly
    mshq_end_date_check[i, "THRESHOLD"] <- 
      mshq_end_date_check[i, "HOURS"] / mshq_weekly
  }
}

if (nrow(filter(mshq_end_date_check, THRESHOLD > 1.5 | THRESHOLD < 0.5)) > 0) {
  
  mshq_threshold_error <- mshq_end_date_check %>%
    filter(THRESHOLD > 1.5 | THRESHOLD < 0.5)
  
  # print end dates outside of threshold
  kable(mshq_threshold_error, 
        caption = "The following MSHQ end dates have hours outside the threshold") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
  } else {
    print("All MSHQ end dates have hours within threshold")
}

# set up end date df for MSHQ check
bislr_end_dates <- setNames(data.frame(matrix(ncol = 1, nrow = 0)), c("END_DATE"))
bislr_end_dates <- bislr_end_dates %>% mutate(END_DATE = as.Date(END_DATE))
i_bislr <- bislr_first_end_date
while (i_bislr != bislr_max_end_date) {
  r <- nrow(bislr_end_dates)
  
  bislr_end_dates[r + 1, 1] <- i_bislr
  
  i_bislr <- i_bislr + 7
}

# join bislr end dates with BISLR data table to get hours for each end date
bislr_end_date_check <- bislr_end_dates %>%
  left_join(tbl(con_prod, "LPM_DATA_BISLR_ORACLE") %>%
              group_by(END_DATE) %>%
              summarise(HOURS = sum(WD_HOURS, na.rm = TRUE)) %>%
              collect(),
            by = c("END_DATE" = "END_DATE")) %>%
  arrange(END_DATE)

# check for each end date to have hours
if (nrow(filter(bislr_end_date_check, is.na(HOURS))) > 0) {
  bislr_missing_dates <- filter(bislr_end_date_check, is.na(HOURS)) %>%
    select(END_DATE) %>%
    pull()
  
  print(paste(bislr_missing_dates, "do not have hours"))
}

# check that the hours total for each end date is within threshold
bislr_end_date_check <- bislr_end_date_check %>%
  mutate(EXPECTED = as.numeric(NA),
         THRESHOLD = as.numeric(NA))
for (i in 1:nrow(mshq_end_date_check)) {
  if (i %% 2 == 1) {
    bislr_end_date_check[i, "EXPECTED"] <- bislr_biweekly
    bislr_end_date_check[i, "THRESHOLD"] <- 
      bislr_end_date_check[i, "HOURS"] / bislr_biweekly
  } else {
    bislr_end_date_check[i, "EXPECTED"] <- bislr_weekly
    bislr_end_date_check[i, "THRESHOLD"] <- 
      bislr_end_date_check[i, "HOURS"] / bislr_weekly
  }
}

if (nrow(filter(bislr_end_date_check, THRESHOLD > 1.5 | THRESHOLD < 0.5)) > 0) {
  
  bislr_threshold_error <- bislr_end_date_check %>%
    filter(THRESHOLD > 1.5 | THRESHOLD < 0.5)
  
  # print end dates outside of threshold
  kable(bislr_threshold_error, 
        caption = "The following BISLR end dates have hours outside the threshold") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
  } else {
    print("All BISLR end dates have hours within threshold")
}

print("End Date check successful")
```

# Merge Schemas
### Merge Mapping Tables
```{r Merge Mapping Tables , echo=FALSE}
# merge statement jobcode mapping table
jobcode_merge <- glue(
  "MERGE INTO LPM_MAPPING_JOBCODE destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_JOBCODE source
   ON (destination.JOBCODE = source.JOBCODE AND
       destination.PAYROLL = source.PAYROLL)
   WHEN MATCHED THEN
   UPDATE SET destination.JOBCODE_DESCRIPTION = source.JOBCODE_DESCRIPTION,
              destination.PROVIDER = source.PROVIDER,
              destination.JOBCODE_PREMIER = source.JOBCODE_PREMIER,
              destination.JOBCODE_PREMIER_DESCRIPTION = source.JOBCODE_PREMIER_DESCRIPTION
   WHEN NOT MATCHED THEN
   INSERT (destination.JOBCODE,
           destination.PAYROLL,
           destination.JOBCODE_DESCRIPTION,
           destination.PROVIDER,
           destination.JOBCODE_PREMIER,
           destination.JOBCODE_PREMIER_DESCRIPTION)
   VALUES (source.JOBCODE,
           source.PAYROLL,
           source.JOBCODE_DESCRIPTION,
           source.PROVIDER,
           source.JOBCODE_PREMIER,
           source.JOBCODE_PREMIER_DESCRIPTION);"
)


# merge statement for paycode mapping table
paycode_merge <- glue(
  "MERGE INTO LPM_MAPPING_PAYCODE destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_PAYCODE source
   ON (destination.PAYCODE_RAW = source.PAYCODE_RAW)
   WHEN MATCHED THEN
   UPDATE SET destination.PAYCODE_PREMIER = source.PAYCODE_PREMIER,
              destination.PAYCODE_DESCRIPTION = source.PAYCODE_DESCRIPTION,
              destination.PAYCODE_CATEGORY = source.PAYCODE_CATEGORY,
              destination.INCLUDE_HOURS = source.INCLUDE_HOURS,
              destination.INCLUDE_EXPENSES = source.INCLUDE_EXPENSES,
              destination.WORKED_PAYCODE = source.WORKED_PAYCODE
   WHEN NOT MATCHED THEN
   INSERT (destination.PAYCODE_RAW,
           destination.PAYCODE_PREMIER,
           destination.PAYCODE_DESCRIPTION,
           destination.PAYCODE_CATEGORY,
           destination.INCLUDE_HOURS,
           destination.INCLUDE_EXPENSES,
           destination.WORKED_PAYCODE)
   VALUES (source.PAYCODE_RAW,
           source.PAYCODE_PREMIER,
           source.PAYCODE_DESCRIPTION,
           source.PAYCODE_CATEGORY,
           source.INCLUDE_HOURS,
           source.INCLUDE_EXPENSES,
           source.WORKED_PAYCODE);"
)

# merge statement for paycycle mapping table
paycycle_merge <- glue(
  "MERGE INTO LPM_MAPPING_PAYCYCLE destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_PAYCYCLE source
   ON (destination.PAYCYCLE_DATE = source.PAYCYCLE_DATE)
   WHEN MATCHED THEN
   UPDATE SET destination.PP_START_DATE = source.PP_START_DATE,
              destination.PP_END_DATE = source.PP_END_DATE,
              destination.PREMIER_DISTRIBUTION = source.PREMIER_DISTRIBUTION
   WHEN NOT MATCHED THEN
   INSERT (destination.PAYCYCLE_DATE,
           destination.PP_START_DATE,
           destination.PP_END_DATE,
           destination.PREMIER_DISTRIBUTION)
   VALUES (source.PAYCYCLE_DATE,
           source.PP_START_DATE,
           source.PP_END_DATE,
           source.PREMIER_DISTRIBUTION);"
) 

# merge statement for cost center mapping table
costcenter_merge <- glue(
  "MERGE INTO LPM_MAPPING_COST_CENTER destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_COST_CENTER source
   ON (destination.ORACLE_COST_CENTER = source.ORACLE_COST_CENTER)
   WHEN MATCHED THEN 
   UPDATE SET destination.LEGACY_COST_CENTER = source.LEGACY_COST_CENTER,
              destination.COST_CENTER_DESCRIPTION = source.COST_CENTER_DESCRIPTION,
              destination.DEFINITION_CODE = source.DEFINITION_CODE
   WHEN NOT MATCHED THEN
   INSERT (destination.ORACLE_COST_CENTER,
           destination.LEGACY_COST_CENTER,
           destination.COST_CENTER_DESCRIPTION,
           destination.DEFINITION_CODE)
   VALUES (source.ORACLE_COST_CENTER,
           source.LEGACY_COST_CENTER,
           source.COST_CENTER_DESCRIPTION,
           source.DEFINITION_CODE);"
)

# merge statement for reporting definition mapping table
repdef_merge <- glue(
  "MERGE INTO LPM_MAPPING_REPDEF destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_REPDEF source
   ON (destination.DEFINITION_CODE = source.DEFINITION_CODE)
   WHEN MATCHED THEN 
   UPDATE SET destination.DEFINITION_NAME = source.DEFINITION_NAME,
              destination.SITE = source.SITE,
              destination.CORPORATE_SERVICE_LINE = source.CORPORATE_SERVICE_LINE,
              destination.VP = source.VP,
              destination.DEPARTMENT_BREAKDOWN = source.DEPARTMENT_BREAKDOWN
   WHEN NOT MATCHED THEN
   INSERT (destination.DEFINITION_CODE,
           destination.DEFINITION_NAME,
           destination.SITE,
           destination.CORPORATE_SERVICE_LINE,
           destination.VP,
           destination.DEPARTMENT_BREAKDOWN)
   VALUES (source.DEFINITION_CODE,
           source.DEFINITION_NAME,
           source.SITE,
           source.CORPORATE_SERVICE_LINE,
           source.VP,
           source.DEPARTMENT_BREAKDOWN);"
)

# merge statement for reporting definition mapping table
keyvol_merge <- glue(
  "MERGE INTO LPM_MAPPING_KEY_VOLUME destination
   USING OAO_DEVELOPMENT.LPM_MAPPING_KEY_VOLUME source
   ON (destination.DEFINITION_CODE = source.DEFINITION_CODE AND
       destination.KEY_VOLUME = source.KEY_VOLUME)
   WHEN MATCHED THEN 
   UPDATE SET destination.CLOSED = source.CLOSED
   WHEN NOT MATCHED THEN
   INSERT (destination.DEFINITION_CODE,
           destination.KEY_VOLUME,
           destination.CLOSED)
   VALUES (source.DEFINITION_CODE,
           source.KEY_VOLUME,
           source.CLOSED);"
)

# delete any definiton codes that were removed or changed in OAO_DEV
repdef_delete <- glue(
  "DELETE FROM LPM_MAPPING_REPDEF
   WHERE DEFINITION_CODE NOT IN 
   (SELECT DEFINITION_CODE FROM OAO_DEVELOPMENT.LPM_MAPPING_REPDEF);"
)

costcenter_delete <- glue(
  "DELETE FROM LPM_MAPPING_COST_CENTER
   WHERE DEFINITION_CODE NOT IN 
   (SELECT DEFINITION_CODE FROM OAO_DEVELOPMENT.LPM_MAPPING_COST_CENTER) OR
         ORACLE_COST_CENTER NOT IN (SELECT ORACLE_COST_CENTER FROM OAO_DEVELOPMENT.LPM_MAPPING_COST_CENTER);"
)

keyvol_delete <- glue(
  "DELETE FROM LPM_MAPPING_KEY_VOLUME
   WHERE DEFINITION_CODE || KEY_VOLUME NOT IN
   (SELECT DEFINITION_CODE || KEY_VOLUME FROM OAO_DEVELOPMENT.LPM_MAPPING_KEY_VOLUME);"
)

# execute mapping table merge statements
con_prod <- dbConnect(odbc(), prod_dsn)
dbBegin(con_prod)
# execute statements and if there is an error  with one of them rollback changes
tryCatch({
  dbExecute(con_prod, jobcode_merge)
  print("LPM_MAPPING_JOBCODE has been merged")
  dbExecute(con_prod, paycode_merge)
  print("LPM_MAPPING_PAYCODE has been merged")
  dbExecute(con_prod, paycycle_merge)
  print("LPM_MAPPING_PAYCYCLE has been merged")
  dbExecute(con_prod, costcenter_merge)
  print("LPM_MAPPING_COST_CENTER has been merged")
  dbExecute(con_prod, costcenter_delete)
  print("Old codes have been removed from LPM_MAPPING_COST_CENTER")
  dbExecute(con_prod, repdef_merge)
  print("LPM_MAPPING_REPDEF has been merged")
  dbExecute(con_prod, repdef_delete)
  print("Old codes have been removed from LPM_MAPPING_REPDEF")
  dbExecute(con_prod, keyvol_merge)
  print("LPM_MAPPING_KEY_VOLUME has been merged")
  dbExecute(con_prod, keyvol_delete)
  print("Old codes have been removed from LPM_MAPPING_KEY_VOLUME")
  dbCommit(con_prod)
  dbDisconnect(con_prod)
  }, 
  error = function(err){
    dbRollback(con_prod)
    dbDisconnect(con_prod)
    print("Error")
    }
  )
print("Mapping Table Merge Successful")
```
### Definition Code Quality Check
```{r Definition Code Quality Chec , echo=FALSE}
# connect to OAO_PRODUCTION
con_prod <- dbConnect(odbc(), prod_dsn)

# read in complete contents of all the DEFINITION_CODE mapping tables
repdef <- tbl(con_prod, "LPM_MAPPING_REPDEF") %>%
  mutate(REPDEF = "Included") %>%
  collect()
costcenter <- tbl(con_prod, "LPM_MAPPING_COST_CENTER") %>%
  mutate(COSTCENTER = "Included") %>%
  collect()
keyvol <- tbl(con_prod, "LPM_MAPPING_KEY_VOLUME") %>%
  mutate(KEYVOL = "Included") %>%
  collect()

# get list of unique definition codes and whether they appear in each table
definition_codes <- as.data.frame(c(repdef$DEFINITION_CODE,
                                    costcenter$DEFINITION_CODE,
                                    keyvol$DEFINITION_CODE)) %>%
  setNames("DEFINITION_CODE") %>%
  distinct() %>%
  left_join(select(repdef, DEFINITION_CODE, REPDEF)%>%
              distinct(),
            by = c("DEFINITION_CODE" = "DEFINITION_CODE")) %>%
  left_join(select(costcenter, DEFINITION_CODE, COSTCENTER)%>%
              distinct(),
            by = c("DEFINITION_CODE" = "DEFINITION_CODE")) %>%
  left_join(select(keyvol, DEFINITION_CODE, KEYVOL) %>%
              distinct(),
            by = c("DEFINITION_CODE" = "DEFINITION_CODE")) %>%
  filter(is.na(REPDEF) |
         is.na(COSTCENTER) |
         is.na(KEYVOL)) %>%
  replace_na(list(REPDEF = "",
                  COSTCENTER = "",
                  KEYVOL = ""))

# print results in table form for the user
if(!is.null(definition_codes)) {
  kable(definition_codes, 
        caption = "The following definition codes are not present in all 3 mapping tables:") %>% 
    kable_styling(latex_options = "striped", 
                  font_size = 10) %>%
    row_spec(0, font_size = 13)
}

print("Definition Code Quality Check Successful")
```

### Update MSHQ Data Tables
```{r Update MSHQ Oracle Data Tables , echo=FALSE}
# establish connection to OAO Development
con_prod <- dbConnect(odbc(), prod_dsn)
con_dev <- dbConnect(odbc(), dev_dsn)

# establish destination table
mshq_destination_table <- "LPM_DATA_MSHQ_ORACLE"
mshq_destination_schema <- "OAO_PRODUCTION"

# identify what files are present in dev schema and not production
mshq_inserts_diff <- setdiff(tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull(),
                        tbl(con_prod, "LPM_DATA_MSHQ_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull())

if (length(mshq_inserts_diff) > 0) {
  # prepare the insert df based on files not present in prod
  mshq_insert_df <- tbl(con_dev, "LPM_DATA_MSHQ_ORACLE") %>%
    filter(FILE_NAME %in% mshq_inserts_diff) %>%
    collect() %>%
    mutate(POSITION_CODE = replace_na(POSITION_CODE, ""),
           POSITION_CODE_DESCRIPTION = replace_na(POSITION_CODE_DESCRIPTION, ""),
           LOCATION_DESCRIPTION = replace_na(LOCATION_DESCRIPTION, ""),
           REVERSE_MAP_HOME = replace_na(REVERSE_MAP_HOME, ""),
           REVERSE_MAP_WORKED = replace_na(REVERSE_MAP_WORKED, ""),
           HOME_DEPARTMENT_NAME = gsub("\'", "''", HOME_DEPARTMENT_NAME),
           WORKED_DEPARTMENT_NAME = gsub("\'", "''", WORKED_DEPARTMENT_NAME),
           POSITION_CODE_DESCRIPTION = gsub("\'", "''", POSITION_CODE_DESCRIPTION),
           EMPLOYEE_NAME = gsub("\'", "''", EMPLOYEE_NAME),
           LOCATION_DESCRIPTION = gsub("\'", "''", LOCATION_DESCRIPTION)) %>%
    mutate(across(everything(), as.character))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts <- 
    lapply(
      lapply(
        lapply(split(mshq_insert_df, 
                     1:nrow(mshq_insert_df)),
               as.list),
        as.character),
      FUN = get_values, mshq_destination_schema, mshq_destination_table)
  
  # create batches of inserts for insert statements
  chunk_length <- 250
  split_queries <- split(inserts, ceiling(seq_along(inserts)/chunk_length))
  
  # append each batch of inserts to batch insert list
  split_queries_values <- list()
  for (i in 1:length(split_queries)) {
    row <- glue_collapse(split_queries[[i]], sep = "\n\n")
    values <- glue('INSERT ALL
                 {row}
                 SELECT 1 from DUAL;')
    split_queries_values <- append(split_queries_values, values)
  }
  
  # execute parallel inserts of 250 record chunks
  registerDoParallel()
  outputPar <- foreach(i = 1:length(split_queries_values), 
                       .packages = c("DBI", "odbc")) %dopar% {
                         con_prod <- dbConnect(odbc(), prod_dsn)
                         tryCatch({
                           dbBegin(con_prod)
                           dbExecute(con_prod, split_queries_values[[i]])
                           dbCommit(con_prod)
                           dbDisconnect(con_prod)
                         },
                         error = function(err){
                           print("error")
                           dbRollback(con_prod)
                           dbDisconnect(con_prod)
                         })
                       }
  registerDoSEQ()
  cat(paste(mshq_inserts_diff, "has been added to", mshq_destination_schema))
} else {
  cat(paste(mshq_destination_schema, "is already up to date"))
}

```

### Update BISLR Data Tables
```{r Update BISLR Oracle Data Tables , echo=FALSE}
# establish connection to OAO Development
con_prod <- dbConnect(odbc(), prod_dsn)
con_dev <- dbConnect(odbc(), dev_dsn)

# establish destination table
bislr_destination_table <- "LPM_DATA_BISLR_ORACLE"
bislr_destination_schema <- "OAO_PRODUCTION"

# identify what files are present in dev schema and not production
bislr_inserts_diff <- setdiff(tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull(),
                        tbl(con_prod, "LPM_DATA_BISLR_ORACLE") %>%
                          select(FILE_NAME) %>%
                          distinct() %>%
                          collect() %>%
                          pull())

if (length(bislr_inserts_diff) > 0) {
  # prepare the insert df based on files not present in prod
  bislr_insert_df <- tbl(con_dev, "LPM_DATA_BISLR_ORACLE") %>%
    filter(FILE_NAME %in% bislr_inserts_diff) %>%
    collect() %>%
    mutate(POSITION_CODE = replace_na(POSITION_CODE, ""),
           POSITION_CODE_DESCRIPTION = replace_na(POSITION_CODE_DESCRIPTION, ""),
           LOCATION_DESCRIPTION = replace_na(LOCATION_DESCRIPTION, ""),
           REVERSE_MAP_HOME = replace_na(REVERSE_MAP_HOME, ""),
           REVERSE_MAP_WORKED = replace_na(REVERSE_MAP_WORKED, ""),
           HOME_DEPARTMENT_NAME = gsub("\'", "''", HOME_DEPARTMENT_NAME),
           WORKED_DEPARTMENT_NAME = gsub("\'", "''", WORKED_DEPARTMENT_NAME),
           POSITION_CODE_DESCRIPTION = gsub("\'", "''", POSITION_CODE_DESCRIPTION),
           EMPLOYEE_NAME = gsub("\'", "''", EMPLOYEE_NAME),
           LOCATION_DESCRIPTION = gsub("\'", "''", LOCATION_DESCRIPTION)) %>%
    mutate(across(everything(), as.character))
  
  # convert the each record/row of tibble to INTO clause of insert statment
  inserts <- 
    lapply(
      lapply(
        lapply(split(bislr_insert_df, 
                     1:nrow(bislr_insert_df)),
               as.list),
        as.character),
      FUN = get_values, bislr_destination_schema, bislr_destination_table)
  
  # create batches of inserts for insert statements
  chunk_length <- 250
  split_queries <- split(inserts, ceiling(seq_along(inserts)/chunk_length))
  
  # append each batch of inserts to batch insert list
  split_queries_values <- list()
  for (i in 1:length(split_queries)) {
    row <- glue_collapse(split_queries[[i]], sep = "\n\n")
    values <- glue('INSERT ALL
                 {row}
                 SELECT 1 from DUAL;')
    split_queries_values <- append(split_queries_values, values)
  }
  
  # execute parallel inserts of 250 record chunks
  registerDoParallel()
  outputPar <- foreach(i = 1:length(split_queries_values), 
                       .packages = c("DBI", "odbc")) %dopar% {
                         con_prod <- dbConnect(odbc(), prod_dsn)
                         tryCatch({
                           dbBegin(con_prod)
                           dbExecute(con_prod, split_queries_values[[i]])
                           dbCommit(con_prod)
                           dbDisconnect(con_prod)
                         },
                         error = function(err){
                           print("error")
                           dbRollback(con_prod)
                           dbDisconnect(con_prod)
                         })
                       }
  registerDoSEQ()
  cat(paste(bislr_inserts_diff, "has been added to", bislr_destination_schema))
} else {
  cat(paste(bislr_destination_schema, "is already up to date"))
}
```

### Update File Dates Tables
```{r Update File Dates Tables , echo=FALSE}
# establish connection to OAO Development
con_dev <- dbConnect(odbc(), dev_dsn)

# add any new files to the mshq dates files
mshq_dates <- tbl(con_dev, "LPM_FILE_DATES_MSHQ") %>%
  collect() %>%
  mutate(RAW_MTIME = as.character(RAW_MTIME)) 
bislr_dates <- tbl(con_dev, "LPM_FILE_DATES_BISLR") %>%
  collect() %>%
  mutate(RAW_MTIME = as.character(RAW_MTIME))
  
# send new dates files to DB
dates_schema <- "OAO_PRODUCTION"
dates_table_mshq <- "LPM_FILE_DATES_MSHQ"
dates_table_bislr <- "LPM_FILE_DATES_BISLR"

# truncate query
dates_truncate_mshq <- glue("TRUNCATE TABLE LPM_FILE_DATES_MSHQ")
dates_truncate_bislr <- glue("TRUNCATE TABLE LPM_FILE_DATES_BISLR")

#replace NA
  mshq_dates <- mshq_dates %>%
    mutate(across(everything(), as.character)) %>%
    mutate(RAW_FILE = replace_na(RAW_FILE, ""),
           DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
           INSERT_FILE = replace_na(INSERT_FILE, ""),
           RAW_MTIME = replace_na(RAW_MTIME, ""),
           START_DATE = replace_na(START_DATE, ""),
           END_DATE = replace_na(END_DATE, ""))

  bislr_dates <- bislr_dates %>%
    mutate(across(everything(), as.character)) %>%
    mutate(RAW_FILE = replace_na(RAW_FILE, ""),
           DECRYPT_FILE = replace_na(DECRYPT_FILE, ""),
           INSERT_FILE = replace_na(INSERT_FILE, ""),
           RAW_MTIME = replace_na(RAW_MTIME, ""),
           START_DATE = replace_na(START_DATE, ""),
           END_DATE = replace_na(END_DATE, ""))

# convert the each record/row of tibble to INTO clause of insert statment
inserts_dates_mshq <- 
  lapply(
    lapply(
      lapply(split(mshq_dates,
                   1:nrow(mshq_dates)),
             as.list),
      as.character),
    FUN = get_values_dates, dates_schema, dates_table_mshq)

# colapse list elements into indvidual insert statements
values_dates_mshq <- glue_collapse(inserts_dates_mshq, sep = "\n\n")

# combine all insert statements into an insert all statement
all_data_mshq_dates <- glue('INSERT ALL
                            {values_dates_mshq}
                            SELECT 1 from DUAL;')

# convert the each record/row of tibble to INTO clause of insert statment
inserts_dates_bislr <- 
  lapply(
    lapply(
      lapply(split(bislr_dates,
                   1:nrow(bislr_dates)),
             as.list),
      as.character),
    FUN = get_values_dates, dates_schema, dates_table_bislr)

# colapse list elements into indvidual insert statements
values_dates_bislr <- glue_collapse(inserts_dates_bislr, sep = "\n\n")

# combine all insert statements into an insert all statement
all_data_bislr_dates <- glue('INSERT ALL
                             {values_dates_bislr}
                             SELECT 1 from DUAL;')

# truncate destination tables before inserts
con_prod <- dbConnect(odbc(), prod_dsn)
dbBegin(con_prod)
tryCatch({
  dbExecute(con_prod, dates_truncate_mshq)
  dbExecute(con_prod, dates_truncate_bislr)
  dbExecute(con_prod, all_data_mshq_dates)
  dbExecute(con_prod, all_data_bislr_dates)
  dbCommit(con_prod)
  dbDisconnect(con_prod)
  print("MSHQ File Dates table has been updated")
  print("BISLR File Dates table has been updated")
  }, 
  error = function(err){
    dbRollback(con_prod)
    dbDisconnect(con_prod)
    print("Error")
    }
  )

```
